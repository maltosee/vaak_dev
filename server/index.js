// Sanskrit Tutor Backend Server with Raw WebSockets
const express = require('express');
const http = require('http');
const WebSocket = require('ws');
const cors = require('cors');
const multer = require('multer');
const path = require('path');
const config = require('./utils/config');
const voicePipeline = require('./modules/pipeline');
const sessionManager = require('./modules/session');
const pollyTTS = require('./modules/tts');

// Initialize Express app
const app = express();
const server = http.createServer(app);


console.log('üì¶ process.env.MAX_SESSIONS =', process.env.MAX_SESSIONS);  // ‚¨ÖÔ∏è Add here
console.log('üì¶ MAX_SESSIONS from config =', config.session.maxSessions);


// Configure WebSocket Server
const wss = new WebSocket.Server({ 
  server,
  perMessageDeflate: false, // Better for audio streaming
  maxPayload: 10 * 1024 * 1024 // 10MB for audio files
});

// Middleware
app.use(cors({
  origin: config.cors.origin,
  credentials: config.cors.credentials
}));
app.use(express.json({ limit: '10mb' }));
app.use(express.static('public'));

// Configure multer for audio uploads
const upload = multer({
  storage: multer.memoryStorage(),
  limits: {
    fileSize: 10 * 1024 * 1024 // 10MB limit
  },
  fileFilter: (req, file, cb) => {
    const allowedMimes = ['audio/webm', 'audio/wav', 'audio/mp3', 'audio/ogg'];
    cb(null, allowedMimes.includes(file.mimetype));
  }
});

// ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
// API Routes 
// ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

// Lightweight health check for Fly.io
app.get('/health-lite', async (req, res) => {
  try {
    const health = await voicePipeline.healthCheck();

    // No logging ‚Äî just return basic result
    const status = health?.overall === true ? 200 : 503;
    res.status(status).send(status === 200 ? 'OK' : 'NOT OK');
  } catch {
    res.status(503).send('NOT OK');
  }
});



// Health check endpoint
app.get('/health', async (req, res) => {
  try {
    const health = await voicePipeline.healthCheck();
    //console.log('ü©∫ HEALTH CHECK:', JSON.stringify(health, null, 2));

    const status = health?.overall === true ? 200 : 503;
    //res.status(status).json(health);
	res.status(status).json({
		  ok: health.overall,
		  timestamp: health.timestamp
		});

	
  } catch (error) {
    console.error('‚ùå /health error:', error.message);
    res.status(500).json({
      overall: false,
      error: error.message,
      timestamp: new Date().toISOString()
    });
  }
});



// Configuration endpoint for client settings
app.get('/config', (req, res) => {
  const clientConfig = {
    vadEndDelayMs: config.stt.vadEndDelayMs,
    enableDualSTT: config.stt.enableDualSTT,
    audioConfig: {
      sampleRate: config.audio.sampleRate,
      channels: config.audio.channels,
      maxDuration: config.audio.maxDuration,
	  minDuration: config.audio.minDuration
    },
    vadConfig: config.vad, // ‚úÖ Now included from secrets
	bargeInCooldownMs: config.audio.bargeInCooldownMs,  // ‚úÖ Add this
	allowBargeTTSPlaybackImmediate: config.audio.allowBargeTTSPlaybackImmediate,
    timestamp: new Date().toISOString()
  };

  console.log('üìã Sending client config:', clientConfig);
  res.json(clientConfig);
});

// Debug endpoint to see all configuration values
app.get('/debug-config', (req, res) => {
  res.json({
    vadConfig: config.vad,
    audioConfig: config.audio,
    sttConfig: config.stt,
    allConfig: config, // This will show everything
    timestamp: new Date().toISOString()
  });
});

// Authentication endpoint
app.post('/auth', (req, res) => {
  try {
    const { name } = req.body;
    const username = name || 'Anonymous User';
    const token = sessionManager.generateToken(username);
    
    console.log(`‚úÖ Token generated for user: ${username}`);
    
    res.json({
      success: true,
      token: token,
      user: username,
      expiresIn: config.session.timeout,
      timestamp: new Date().toISOString()
    });
  } catch (error) {
    console.error('‚ùå Authentication failed:', error.message);
    res.status(500).json({
      success: false,
      error: 'Authentication failed',
      timestamp: new Date().toISOString()
    });
  }
});

// Pipeline statistics endpoint
app.get('/stats', (req, res) => {
  const stats = voicePipeline.getStats();
  res.json({
    ...stats,
    activeSessions: sessionManager.getActiveSessionCount(),
    timestamp: new Date().toISOString()
  });
});

// Text-to-speech endpoint
app.post('/tts', async (req, res) => {
  try {
    const { text, voice, language } = req.body;
    
    if (!text) {
      return res.status(400).json({ error: 'Text is required' });
    }

    const ttsResult = await pollyTTS.synthesizeSpeech(text, {
      voiceId: voice || 'Kajal',
      languageCode: language || 'hi-IN',
      engine: 'neural'
    });

    if (ttsResult.success) {
      res.set({
        'Content-Type': 'audio/mpeg',
        'Content-Length': ttsResult.audioBuffer.length
      });
      res.send(ttsResult.audioBuffer);
    } else {
      res.status(500).json({ error: ttsResult.error });
    }
  } catch (error) {
    console.error('‚ùå TTS endpoint error:', error.message);
    res.status(500).json({ error: 'TTS processing failed' });
  }
});

// File upload endpoint for testing
app.post('/upload-audio', upload.single('audio'), async (req, res) => {
  try {
    if (!req.file) {
      return res.status(400).json({ error: 'No audio file provided' });
    }

    const userId = req.body.userId || 'upload-user';
    const result = await voicePipeline.processVoiceConversation(req.file.buffer, userId);
    
    res.json(result);
  } catch (error) {
    console.error('‚ùå Upload processing error:', error.message);
    res.status(500).json({ error: 'Audio processing failed' });
  }
});

// ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
// WebSocket Handling - Converted to Raw WebSockets
// ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

wss.on('connection', (ws, req) => {
  
  const clientId = generateClientId();
  ws.clientId = clientId;
  
  // Clean up any sessions with dead WebSocket connections
 // sessionManager.cleanupDeadConnections();
  
  // Create new session
  const user = { id: clientId, name: `User-${clientId.substring(0, 8)}` };
  const session = sessionManager.createSession(user,ws); 
  
  session.state = 'listening';  // ‚úÖ Use this instead

  
  console.log('üîó Client connected:', clientId);
  
  // Send configuration to client immediately
  sendMessage(ws, {
    type: 'config',
    vadEndDelayMs: config.stt.vadEndDelayMs,
    enableDualSTT: config.stt.enableDualSTT,
    audioConfig: {
      sampleRate: config.audio.sampleRate,
      channels: config.audio.channels
    }
  });
  
  sendMessage(ws, {
    type: 'connected',
    message: 'WebSocket connection established',
    timestamp: new Date().toISOString()
  });

  // Handle messages
  ws.on('message', async (data) => {
    try {
      if (Buffer.isBuffer(data)) {
        await handleAudioMessage(data, ws);
      } else {
        const parsed = JSON.parse(data.toString());
        await handleTextMessage(parsed, ws);
      }
    } catch (error) {
      console.error('‚ùå WebSocket message error:', error.message);
      sendMessage(ws, {
        type: 'error',
        message: 'Message processing failed'
      });
    }
  });

  // Handle disconnection
  ws.on('close', (code, reason) => {
    console.log('üîå Client disconnected:', clientId, 'Code:', code, 'Reason:', reason.toString());
    // Clean up any associated sessions
    sessionManager.cleanupBySocketId(clientId);
  });

  // Handle connection errors
  ws.on('error', (error) => {
    console.error('‚ùå WebSocket error:', error);
  });
});

/**
 * Generate unique client ID
 */
function generateClientId() {
  return Math.random().toString(36).substring(2, 15) + Math.random().toString(36).substring(2, 15);
}

/**
 * Send JSON message to WebSocket client
 */
function sendMessage(ws, message) {
  if (ws.readyState === WebSocket.OPEN) {
    ws.send(JSON.stringify(message));
  }
}

/**
 * Send binary data to WebSocket client
 */
function sendBinary(ws, buffer) {
  if (ws.readyState === WebSocket.OPEN) {
    ws.send(buffer);
  }
}

/**
 * Handle audio message from WebSocket
 * @param {Buffer} audioBuffer - Audio data from client
 * @param {WebSocket} ws - WebSocket connection
 */
async function handleAudioMessage(audioBuffer, ws) {
  const userId = ws.clientId;
  const session = sessionManager.getSession(ws.clientId);

  console.log(`üé§ Received audio: ${audioBuffer.length} bytes from ${userId}`);

  if (!userId) {
    console.warn(`‚ö†Ô∏è Received audio from client with no ID. Ignoring.`);
    return;
  }

  if (!session || session.state !== 'listening') {
    console.log(`‚ö†Ô∏è Barge-in attempt detected for ${userId}. Current state: ${session?.state}. Ignoring audio.`);
    sendMessage(ws, {
      type: 'status_update',
      message: 'Server is currently processing your previous request. Please wait.',
      statusType: 'warning',
      statusCode: 'BUSY_SERVER',
      timestamp: new Date().toISOString()
    });
    return;
  }

  session.state = 'processing';

  try {
    const pipelineResult = await voicePipeline.processVoiceConversation(audioBuffer, userId);

    if (pipelineResult.success) {
      if (pipelineResult.isUnrecognizedLanguage) {
        const errorResponse = {
          type: 'llm_response',
          text: pipelineResult.steps.llm.response,
          transcription: 'Unrecognized Language',
          language: 'unknown',
          processingTime: pipelineResult.totalDuration,
          timestamp: pipelineResult.timestamp
        };

        console.log('üîç DEBUG: About to send unrecognized language response');
        sendMessage(ws, errorResponse);
        console.log('üîç DEBUG: Unrecognized language response sent');

        await new Promise(resolve => setTimeout(resolve, 200));
        const errorTTS = await pollyTTS.synthesizeSpeech(pipelineResult.steps.llm.response, {
          voiceId: 'Kajal',
          languageCode: 'hi-IN',
          engine: 'neural'
        });

        if (errorTTS.success) {
          sendBinary(ws, errorTTS.audioBuffer);
          console.log('üîç DEBUG: Error audio response sent');
        }

        return;
      }

      const response = {
        type: 'llm_response',
        text: pipelineResult.steps.llm.response,
        transcription: pipelineResult.steps.stt.text,
        language: pipelineResult.steps.stt.language,
        processingTime: pipelineResult.totalDuration,
        timestamp: pipelineResult.timestamp,
        debug: config.enableDebugLogging ? pipelineResult.steps.stt.debug : undefined
      };

      console.log('üîç DEBUG: About to send JSON response');
      sendMessage(ws, response);
      console.log('üîç DEBUG: JSON response sent');

      await new Promise(resolve => setTimeout(resolve, 200));
      sendBinary(ws, pipelineResult.steps.tts.audioBuffer);
      console.log('üîç DEBUG: Audio response sent');

      console.log(`‚úÖ Complete conversation processed for ${userId}`);
    } else {
      sendMessage(ws, {
        type: 'error',
        message: `Pipeline processing failed: ${pipelineResult.error}`,
        timestamp: pipelineResult.timestamp
      });
    }

  } catch (error) {
    console.error('‚ùå Audio processing error:', error);
    sendMessage(ws, {
      type: 'error',
      message: 'Audio processing failed',
      timestamp: new Date().toISOString()
    });
  } finally {
    session.state = 'listening'; // ‚úÖ Always reset no matter what
  }
}

/**
 * Handle text message from WebSocket
 * @param {Object} message - Parsed JSON message
 * @param {WebSocket} ws - WebSocket connection
 */
async function handleTextMessage(message, ws) {
  const userId = ws.clientId;
  const session = sessionManager.getSession(ws.clientId);
  
  console.log(`üí¨ Received text message from ${userId}:`, message.type);
  
  try {
    switch (message.type) {
      case 'text_input':
        if (message.text) {
          const result = await voicePipeline.processTextConversation(message.text, userId);
          
          if (result.success) {
            sendMessage(ws, {
              type: 'llm_response',
              text: result.response,
              transcription: result.inputText,
              language: 'text',
              processingTime: result.totalDuration,
              timestamp: result.timestamp
            });
            
            // Send audio if available
            if (result.audioBuffer) {
              await new Promise(resolve => setTimeout(resolve, 200));
              sendBinary(ws, result.audioBuffer);
            }
          } else {
            sendMessage(ws, {
              type: 'error',
              message: result.error
            });
          }
        }
        break;
        
      case 'ping':
        sendMessage(ws, {
          type: 'pong',
          timestamp: new Date().toISOString()
        });
        break;
        
      case 'check_ready':
	  
		 //const session = sessionManager.getSession(ws.userId);
		 const sessionForReady = sessionManager.getSession(ws.clientId);
		 const isReady = sessionForReady?.state === 'listening';
		 
		 sendMessage(ws, {
		  type: 'status_update',
		  statusCode: isReady ? 'READY' : 'BUSY_SERVER',
		  message: isReady ? 'Server ready' : 'Server busy',
		  timestamp: new Date().toISOString()
		});
	  
	    break;
	  
	  case 'get_config':
        sendMessage(ws, {
          type: 'config',
          vadEndDelayMs: config.stt.vadEndDelayMs,
          enableDualSTT: config.stt.enableDualSTT,
          audioConfig: {
            sampleRate: config.audio.sampleRate,
            channels: config.audio.channels
          }
        });
        break;
        
      default:
        console.log(`‚ö†Ô∏è Unknown message type: ${message.type}`);
		 // üëá fallback to session logic for other types
		//const session = sessionManager.getOrCreateSession(ws.userId, ws);
		session.handleMessage(message);
    }
    
  } catch (error) {
    console.error('‚ùå Text message processing error:', error);
    sendMessage(ws, {
      type: 'error',
      message: 'Text message processing failed'
    });
  }
}

// ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
// Error Handling & Cleanup
// ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

// Global error handlers
process.on('uncaughtException', (error) => {
  console.error('üí• Uncaught Exception:', error);
  process.exit(1);
});

process.on('unhandledRejection', (reason, promise) => {
  console.error('üí• Unhandled Rejection at:', promise, 'reason:', reason);
});

// Graceful shutdown
process.on('SIGTERM', () => {
  console.log('üì¥ SIGTERM received, shutting down gracefully');
  server.close(() => {
    console.log('üì¥ Server closed');
    process.exit(0);
  });
});

process.on('SIGINT', () => {
  console.log('üì¥ SIGINT received, shutting down gracefully');
  server.close(() => {
    console.log('üì¥ Server closed');
    process.exit(0);
  });
});

// ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
// Server Startup
// ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

//const PORT = config.server.port;
//const HOST = config.server.host;
const PORT = process.env.PORT || 8080;
const HOST = '0.0.0.0';


server.listen(PORT, HOST, async () => {
  console.log('üöÄ Sanskrit Tutor Backend Started!');
  console.log(`üì° Server: http://${HOST}:${PORT}`);
  console.log(`üîå WebSocket: ws://${HOST}:${PORT}`);
  console.log(`üè• Health: http://${HOST}:${PORT}/health`);
  console.log(`üìã Config: http://${HOST}:${PORT}/config`);
  console.log(`üîê Auth: POST http://${HOST}:${PORT}/auth`);
  console.log(`üéµ TTS: POST http://${HOST}:${PORT}/tts`);
  
  // Display configuration info
  console.log('');
  console.log('üîß Configuration:');
  console.log(`   Dual STT: ${config.stt.enableDualSTT ? '‚úÖ Enabled' : '‚ùå Disabled'}`);
  console.log(`   VAD Delay: ${config.stt.vadEndDelayMs}ms`);
  console.log(`   Custom ASR: ${config.stt.customAsrUrl}`);
  console.log(`   Environment: ${config.server.env}`);
  console.log('');
  
  console.log('Ready for voice conversations! üéØ');
});